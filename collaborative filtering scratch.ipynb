{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration and references --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering on MovieLens using matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with bias\n",
    "$U$ and $V$ are the embedding matrix for users and movies respectively. We are adding $u^0$ which is a bias vector of dimension $n_u$ and $v^0$ which is a bias vector of dimension $n_m$. The predicted rating for $i$ user and $j$ movie is given by the below equation:\n",
    "\n",
    "$$\\hat{y}_{ij} = u_{0i} + v_{0j} + u_i \\cdot v_j  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized version of gradient descent equation:\n",
    "   $$1.\\: u^0 = u^0 - \\eta * \\nabla E(u^0) \\;where \\; \\nabla E(u^0)=-2*np.sum((Y-UV^T-u^0-v^{0T})*R,axis = 0)/N$$ <br/>\n",
    "   $$2.\\: v^0 = v^0 - \\eta * \\nabla E(v^0) \\;where \\; \\nabla E(v^0)=-2*(np.sum((Y-UV^T-u^0-v^{0T})*R,axis = 1))^T/N$$<br/>\n",
    "   \n",
    "   $$3. \\: U = U - \\eta * \\nabla E(U)\\;where \\; \\nabla E(U)=-2*((Y-UV^T-u^0-v^{0T})*R)V)/N$$<br/>\n",
    "   $$4. \\:V = V - \\eta * \\nabla E(V) \\;where \\; \\nabla E(V)=-2*((Y-UV^T-u^0-v^{0T})*R)^TU)/N$$<br/>\n",
    "   \n",
    "   where $R$ is the binarized version of $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_col(col):\n",
    "    \"\"\"\n",
    "    Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx[x] for x in col]), len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df):\n",
    "    \"\"\"\n",
    "    Encodes rating data with continous user and movie ids\n",
    "    \n",
    "    Arguments:\n",
    "      df: a csv file with columns user_id,movie_id,rating \n",
    "    \n",
    "    Returns:\n",
    "      df: a dataframe with the encode data\n",
    "      num_users\n",
    "      num_movies\n",
    "    \"\"\"\n",
    "    _,user_col,num_users = proc_col(df.userId)\n",
    "    _,movie_col,num_movies = proc_col(df.movieId)\n",
    "    df.userId = user_col\n",
    "    df.movieId = movie_col\n",
    "    return df, num_users, num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_new_data(df_val, df_train):\n",
    "    \"\"\" Encodes df_val with the same encoding as df_train.\n",
    "    Returns:\n",
    "    df_val: dataframe with the same encoding as df_train\n",
    "    \"\"\"\n",
    "    u2idx,_,_ = proc_col(df_train.userId)\n",
    "    m2idx,_,_= proc_col(df_train.movieId)\n",
    "    df_val = df_val.loc[df_val['userId'].isin(u2idx.keys())]\n",
    "    df_val = df_val.loc[df_val['movieId'].isin(m2idx.keys())]\n",
    "    df_val.userId = df_val.userId.apply(lambda x: u2idx[x])\n",
    "    df_val.movieId = df_val.movieId.apply(lambda x: m2idx[x])\n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing embeddings matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedings(n, K):\n",
    "    \"\"\" \n",
    "    Create a numpy random matrix of shape n, K which will represent embedding matrix\n",
    "    \n",
    "    The random matrix should be initialized with uniform values in (0, 6/K)\n",
    "    Arguments:\n",
    "    \n",
    "    Inputs:\n",
    "    n: number of items/users\n",
    "    K: number of factors in the embedding \n",
    "    \n",
    "    Returns:\n",
    "    emb: numpy array of shape (n, num_factors)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    emb = 6*np.random.random((n, K)) / K\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Lens data available here: http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20205 19507\n"
     ]
    }
   ],
   "source": [
    "path = \"ml-latest-small/\"\n",
    "data = pd.read_csv(path + \"ratings.csv\")\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()\n",
    "df_train, num_users, num_movies = encode_data(train.copy())\n",
    "df_val = encode_new_data(val.copy(), train.copy())\n",
    "print(len(val), len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50 #dimension of embedding vector\n",
    "emb_user = create_embedings(num_users, K)\n",
    "emb_movie = create_embedings(num_movies, K)\n",
    "bias_user = create_embedings(num_users,1)\n",
    "bias_movie = create_embedings(num_movies,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def df2matrix(df, nrows, ncols, column_name=\"rating\"):\n",
    "    \"\"\" \n",
    "    Returns a sparse matrix constructed from a dataframe. Sparse matrix helps in faster calculation\n",
    "    \n",
    "    This code assumes the df has columns: MovieID,UserID,Rating\n",
    "    \"\"\"\n",
    "    values = df[column_name].values\n",
    "    ind_movie = df['movieId'].values\n",
    "    ind_user = df['userId'].values\n",
    "    return sparse.csc_matrix((values,(ind_user, ind_movie)),shape=(nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-34-a947273f44ec>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-a947273f44ec>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    +bias_user[df['userId'].values]+bias_movie[df['movieId'].values]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def sparse_multiply(df, emb_user, emb_movie,bias_user,bias_movie):\n",
    "    \"\"\"This function returns U*V^T element wise multi by R as a sparse matrix.\n",
    "    \n",
    "    It avoids creating the dense matrix U*V^T\n",
    "    \"\"\"\n",
    "    df[\"Prediction\"] = np.squeeze(np.sum(emb_user[df[\"userId\"].values]*emb_movie[df[\"movieId\"].values], axis=1)[:,None]\n",
    "                                  +bias_user[df['userId'].values]+bias_movie[df['movieId'].values])\n",
    "    return df2matrix(df, emb_user.shape[0], emb_movie.shape[0], column_name=\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(df, emb_user, emb_movie, bias_user, bias_movie):\n",
    "    \"\"\"\n",
    "    Calculate the prediction and then calculate the MSE error\n",
    "    \"\"\"\n",
    "    Y = df2matrix(df, emb_user.shape[0], emb_movie.shape[0])\n",
    "    N = len(Y.data)\n",
    "    df[\"Prediction\"] = np.sum(emb_user[df[\"userId\"].values]*emb_movie[df[\"movieId\"].values], axis=1)[:,None]+bias_user[df['userId'].values]+bias_movie[df['movieId'].values]\n",
    "    error = np.sum(np.square(df.Prediction - df.rating))/N\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_regularized(df, Y, emb_user, emb_movie,bias_user,bias_movie,lambd):\n",
    "    \"\"\" \n",
    "    Computes the gradient. \n",
    "    \n",
    "    Arguments:\n",
    "      df: dataframe with all data or a subset of the data\n",
    "      Y: sparse representation of df\n",
    "      emb_user: embeddings for users\n",
    "      emb_movie: embeddings for movies\n",
    "      bias_user: bias for user\n",
    "      bias_movie: bias for movie\n",
    "      \n",
    "    Returns:\n",
    "      d_emb_user: gradient wrt user embedding\n",
    "      d_emb_movie: gradient wrt movie embedding\n",
    "      d_bias_user: gradient wrt user bias\n",
    "      d_bias_movie: gradient wrt movie bias\n",
    "      lambd: regularization parameter\n",
    "      \n",
    "    \"\"\"\n",
    "    N = len(Y.data) #non-empty count\n",
    "    Y_predict= sparse_multiply(df, emb_user, emb_movie, bias_user, bias_movie) #prediction sparse matrix\n",
    "    delta = (Y - Y_predict).toarray()\n",
    "    \n",
    "    #calculating the gradient\n",
    "    d_emb_user = -2*np.dot(delta,emb_movie)/N + 2*lambd*emb_user/N\n",
    "    d_emb_movie = -2*np.dot(delta.T,emb_user)/N +2*lambd*emb_movie/N\n",
    "    d_bias_user = -2*np.sum(delta,axis=1)/N\n",
    "    d_bias_movie = -2*np.sum(delta,axis=0).T/N\n",
    "    \n",
    "    return d_emb_user,d_emb_movie,d_bias_user,d_bias_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use a for loop to iterate through gradient descent\n",
    "def gradient_descent_reg(df, emb_user, emb_movie,bias_user,bias_movie,iterations=100, learning_rate=0.01, df_val=None,lambd=.01,mom_fact = 0.9):\n",
    "    \"\"\" \n",
    "    Computes gradient descent with momentum (mom_fact =.9 (default)) for a number of iterations.\n",
    "    \n",
    "    Prints training cost and validation cost (if df_val is not None) every 50 iterations.\n",
    "    \n",
    "    Returns: Learnt parameters\n",
    "    emb_user: the trained user embedding\n",
    "    emb_movie: the trained movie embedding\n",
    "    bias_user: bias for user\n",
    "    bias_movie: bias for movie\n",
    "    \"\"\"\n",
    "#     set_trace()\n",
    "    Y = df2matrix(df, emb_user.shape[0], emb_movie.shape[0])\n",
    "    # Weighted gradient initialization\n",
    "    grad_u_moment,grad_m_moment ,grad_ub_moment,grad_mb_moment= gradient_regularized(df,Y,emb_user,emb_movie,bias_user,bias_movie,lambd)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad_user,grad_movie,grad_user_bias,grad_movie_bias = gradient_regularized(df,Y,emb_user,emb_movie,bias_user,bias_movie,lambd)\n",
    "        \n",
    "        #Weighted gradient calculation\n",
    "        grad_u_moment = mom_fact*grad_u_moment+(1-mom_fact)*grad_user #user embedding\n",
    "        grad_m_moment = mom_fact*grad_m_moment + (1-mom_fact)*grad_movie #movie embedding\n",
    "        grad_ub_moment = mom_fact*grad_ub_moment+(1-mom_fact)*grad_user_bias #user bias\n",
    "        grad_mb_moment = mom_fact*grad_mb_moment + (1-mom_fact)*grad_movie_bias  #movie bias      \n",
    "        \n",
    "        #weight update\n",
    "        emb_user = np.array(np.subtract(emb_user,learning_rate*grad_u_moment))\n",
    "        emb_movie = np.array(np.subtract(emb_movie,learning_rate*grad_m_moment))\n",
    "        bias_user = np.array(np.subtract(bias_user,learning_rate*grad_ub_moment))\n",
    "        bias_movie = np.array(np.subtract(bias_movie,learning_rate*grad_mb_moment))\n",
    "        \n",
    "        if df_val is not None and i%50 ==0:\n",
    "            print(\"Training cost:\",cost(df, emb_user, emb_movie,bias_user,bias_movie))\n",
    "            print(\"Validation cost:\",cost(df_val, emb_user, emb_movie,bias_user,bias_movie))\n",
    "\n",
    "    return emb_user, emb_movie, bias_user,bias_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (79799,671) (79799,8442) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-10d6ea463b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_movie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_movie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_movie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_movie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-234ab2a0ba31>\u001b[0m in \u001b[0;36mgradient_descent_reg\u001b[0;34m(df, emb_user, emb_movie, bias_user, bias_movie, iterations, learning_rate, df_val, lambd, mom_fact)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training cost:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_movie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias_movie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation cost:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_movie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias_movie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-411d26a275eb>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(df, emb_user, emb_movie, bias_user, bias_movie)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memb_movie\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbias_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbias_movie\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (79799,671) (79799,8442) "
     ]
    }
   ],
   "source": [
    "emb_user, emb_movie, bias_user, bias_movie = gradient_descent_reg(df_train, emb_user, emb_movie, bias_user, bias_movie, iterations=1, learning_rate=1, df_val=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
